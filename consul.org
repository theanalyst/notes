#+TITLE: Service Discovery & Consul
#+AUTHOR: Abhishek L
#+EMAIL: @abhishekl
#+REVEAL_TRANS: none
#+OPTIONS: reveal_progress toc:nil num:nil
#+REVEAL_MIN_SCALE: 0.01
#+REVEAL_MARGIN: 0.1
#+REVEAL_ROOT: ./reveal.js-3.0.0
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_THEME: white
#+REVEAL_HLEVEL: 2
* Service Discovery
** Basic Idea
- Identify running services, available services
- Know host,port... for binding to services, eg. DB etc
- Also encompasses Service Registration involving a service
  registering its details
- Necessary for typical SOA, multi tier apps etc.

** Examples
- DNS - The internet scale website (service) discovery
- ~/etc/services~

*** Simple app
#+caption: simple webapp
#+begin_src ditaa :file images/simple.png -r -S

                       +------------+
                       |   auth     |
                       |            |
                       +------------+
  +-----------+        +------------+     +-----------+
  |           |        |   req      |     |           |
  |  webapp   |        |  processor |     |   DB      |
  +-----------+        +------------+     +-----------+

#+end_src

#+REVEAL: split

Discovery! which nodes do processing?

#+begin_src ditaa :file images/simple2.png -r -S

                                  
  +-----------+        +------------+
  |           |        |            |
  |  webapp   |        |  req proc  |
  +-----------+        +------------+


#+end_src

#+REVEAL: split

#+begin_src ditaa :file images/simple3.png -r -S

                                              +------+
                                              |  proc|
  +-----------+        +------------+         +------+
  |           |        |            |         +------+
  |  webapp   |        |  LB        |         |  proc|
  +-----------+        +------------+         +------+
                                              +------+
                                              |  proc|
                                              +------+


#+end_src

Balance requests among different nodes with LB, however, an
anti-pattern! As this now introduces a single point of failure

** Problems
- Service Discovery?
- Health Checking
- Configuration Management: whether a configuration file change can be
  reflected in all nodes easily?
  
** Question
How do various services sync on their activities & come to agree on
the basic information about their environment


* Chubby
- http://research.google.com/archive/chubby.html
- Not an algo paper, very much an engineering paper.  In fact
  excellent use of algorithmic concepts & create a practical system
  usable by many apps

#+begin_quote
Building Chubby was an engineering effort required to fill the needs
mentioned above; it was not research. We claim no new algorithms or
techniques. The purpose of this paper is to describe what we did and
why, rather than to advocate it.
#+end_quote

** Basic Idea
- Distributed lock server
- Allow leader to discover clients
- Allow clients to elect leaders
- Provided a K-V store
- Coarse grained lock service for loosely coupled distributed systems
- At its core, Paxos (Sort of a Paxos as a Service thing)
- Later an inspiration for ZK, doozerd, etcd & now consul

** Key design ideas
- Lock server, as opposed to providing a library for say, Paxos
- Small filestore kind of service for clients to share information

 
* Consul
- Opiniated service discovery tool
- Abstractions for service discovery, monitoring
- Provides HTTP & DNS interfaces
- Agents running on every node, servers additinaly host the KV store and participate in raft
- DNS interface allows existing services to use consul without any modifications

** Architecture
TBD

*** KV Store
**** Consistency
CP model, ensured via Raft consensus by consul servers
Three modes
***** default  
- all reads go through the leader
- However in a network partition, potential stale value possible for reads only. 
- Primarily because reads are serviced by a leader without commiting into the raft log yet
- Sort of performance tradeoff, against consistency (for reads only)
  as this otherwise involves a round trip to the leader in quorum
- Sort of default for etcd. 
- Writes are guaranteed to be consistent, so are reads after writes. 

***** consistent
- all reads go through leader, also there is one more round trip to
ensure that the leader in quorum only services read. 
- Truly consistent mode
- Potentially useful for things like leader election which must be truly consistent

***** stale 
- Any server node services a read
- Potential stale value possible within 50ms of the leader
- Tradeoff : fast & scaleable reads (this is default in k-v stores like etcd)

**** Leader Election
- Leverage K-V store to aid in leader elections
- Use an agreed upon key, of sort ~service/<service-name>/leader~
- Use of sessions (Similar to Chubby Locks.. Advisory not Mandatory)

*** Health checks
- Simple script on nodes running agents.
- Need to provide nagios style 0,1,2 status
- Consul combines a service level health checks + Serf, a gossip based protocol to check for agents

** Demo
**** Rest API
#+begin_src restclient
#-*- restclient -*-
# K-V store 
# index
GET http://localhost:8500/v1/kv/?recurse


# Get a value from the k-v store
# also consistent? and stale? modes
GET http://localhost:8500/v1/kv/foo

  
# update the same value?
PUT http://localhost:8500/v1/kv/foo
ContentType: text/json
barbar

# check & set
PUT http://localhost:8500/v1/kv/foo?cas=2834
bar

# Wait for change (and for a time of 120s)
# (setq restclient-same-buffer-response t)
GET http://localhost:8500/v1/kv/foo?index=2866&wait=120s

# Services
GET http://localhost:8500/v1/agent/services


# Incomplete
# Session
PUT http://localhost:8500/v1/session/create
{
  "LockDelay": "60s",
  "Name": "ceph-service-lock",
  "Node": "node2",
  "Checks": ["service:ceph"]
}
  

## Leader Election
PUT http://localhost:8500/v1/kv/service/ceph/leader?d8b758ac-6810-a093-a839-76ee0969898c
node2

# Watch for the key changes
GET http://localhost:8500/v1/kv/service/ceph/leader

# Step down
PUT http://localhost:8500/v1/kv/service/ceph/leader?release=3eeab2e5-4c1a-fd92-8beb-9fcc795d8ee6
#+end_src     
