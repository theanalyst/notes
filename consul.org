#+TITLE: Service Discovery & Consul
#+AUTHOR: Abhishek L
#+EMAIL: @abhishekl
#+REVEAL_TRANS: none
#+OPTIONS: reveal_progress toc:nil num:nil
#+REVEAL_MIN_SCALE: 0.01
#+REVEAL_MARGIN: 0.1
#+REVEAL_ROOT: ./reveal.js-3.0.0
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_THEME: white
#+REVEAL_HLEVEL: 2
* Service Discovery
** Basic Idea
- Identify running services, available services
- Know host,port... for binding to services, eg. DB etc
- Also encompasses Service Registration involving a service
  registering its details
- Necessary for typical SOA, multi tier apps etc.

** Examples
- DNS - The internet scale website (service) discovery
- ~/etc/services~

*** Simple app
#+caption: simple webapp
#+header: :exports results
#+begin_src ditaa :file images/simple.png :cmdline -r -S

                       +------------+
                       |   auth     |
                       |            |
                       +------------+


  +-----------+        +------------+     +-----------+
  |           |        |   req      |     |           |
  |  webapp   |        |  processor |     |   DB      |
  +-----------+        +------------+     +-----------+

#+end_src

#+REVEAL: split

Discovery! which nodes do processing?

#+caption: Processing
#+header: :exports results
#+begin_src ditaa :file images/simple2.png :cmdline -r -S


  +-----------+        +------------+
  |           |        |            |
  |  webapp   |        |  req proc  |
  +-----------+        +------------+


#+end_src
#+caption: a typical solution
#+header: :exports results
#+begin_src ditaa :file images/simple3.png :cmdline -r -S

                                              +------+
                                              |  proc|
  +-----------+        +------------+         +------+
  |           |        |            |
  |           |        |            |         +------+
  |  webapp   |        |  LB        |         |  proc|
  +-----------+        +------------+         +------+

                                              +------+
                                              |  proc|
                                              +------+


#+end_src

Balance requests among different nodes with LB, however, an
anti-pattern! As this now introduces a single point of failure

** Problems
- Service Discovery?
- Health Checking
- Configuration Management: whether a configuration file change can be
  reflected in all nodes easily?

** Problem Statement
How do various services sync on their activities & come to agree on
the basic information about their environment

* Chubby

- http://research.google.com/archive/chubby.html
- Not an algo paper, very much an engineering paper.  In fact
  excellent use of algorithmic concepts & create a practical system
  usable by many apps

#+begin_quote
Building Chubby was an engineering effort required to fill the needs
mentioned above; it was not research. We claim no new algorithms or
techniques. The purpose of this paper is to describe what we did and
why, rather than to advocate it. -- Mike Burrows, Google
#+end_quote

#+REVEAL: split


- Coarse grained lock service for loosely coupled distributed systems
- At its core, Paxos (Sort of a Paxos as a Service thing)
- Primarily, a distributed lock server
- Later an inspiration for ZK, doozerd, etcd & now consul

** Key design ideas
- Lock server, as opposed to providing a library for say, Paxos
- Small filestore kind of service for clients to share information

** Usecases
- Allow leader to discover clients
- Allow clients to elect leaders
- Provided a K-V store
- Used by GFS to elect master
- Used by BigTable to allow clients to disover master & vice versa

** Locks
We'll revisit this session after leader election
- Coarse grained locks (Fine grained locks are expected to be built by clients)
- Locks are advisory, not mandatory, means locked objects are not inaccessible to other clients.
- ~LockDelay~ parameter used to ensure other services cannot acquire the lock until this time after Lock expiry
- A Sequencer requestable by clients, in order to verify locking, can be attached to requests
* Consul
- Opiniated service discovery tool
- Abstractions for service discovery, monitoring
- Provides HTTP & DNS interfaces
- Agents running on every node, servers additinaly host the KV store and participate in raft
- DNS interface allows existing services to use consul without any modifications

* Architecture
file:images/consul-arch.png

* KV Store
** Consistency
CP model, ensured via Raft consensus by consul servers
Three modes

*** default
- all reads go through the leader, however in a network partition, potential stale value possible for reads only.
- Raft + leader leasing , a small interval of transient time after
  which leader assumes its role stable (300 ms) [1]
- Primarily because reads are serviced by a leader without commiting into the raft log yet
- Sort of performance tradeoff, against consistency (for reads only)
  as this otherwise involves a round trip to the leader in quorum
- Writes are guaranteed to be consistent, as they go through the raft
  logs, so are reads after writes.

*** consistent
- all reads go through leader, also there is one more round trip to
  ensure that the leader in quorum only services read.
- Truly consistent mode

*** stale
- Any server node services a read
- Potential stale value possible within 50ms of the leader
- Tradeoff : fast & scaleable reads (this is default in k-v stores like etcd)

** Leader Election
- Leverage K-V store to aid in leader elections
- Use an agreed upon key, of sort ~service/<service-name>/leader~
- Use of sessions (Similar to Chubby Locks.. Advisory not Mandatory)
- Once a session is created, released when either serf/health check
  fails, service/node dereg or a manual release
- Clients can watch the key (blocking read) and check for the session attribute
- A non existant session attribute implies no leader, and the session
  is up for grabs
#+REVEAL: split
- ~(Key,LockIndex,Session)~ acts as a unique sequencer
- ~LockDelay~ specifiable 

** Health checks
- Intrinsic membership checks with Serf, a gossip based protocol
- Simple script on nodes running agents.
- Need to provide nagios style 0,1,2 status

** Current Deployment
1) Each service registers its ip address as an A record for the address: `<service_name>.service.consul`
2) Each service registers its hostname: `<hostname>.node.consul` as an SRV record for `<service_name>.service.consul`
3) Depending on the service Puppet blocks until an address is resolvable or SRV records are retrievable
4) Fail if a DNS address is not resolvable

** Demo
**** Rest API
#+begin_src restclient
#-*- restclient -*-
# K-V store
# index
GET http://localhost:8500/v1/kv/?recurse


# Get a value from the k-v store
# also consistent? and stale? modes
GET http://localhost:8500/v1/kv/foo


# update the same value?
PUT http://localhost:8500/v1/kv/foo
ContentType: text/json
barbar

# check & set
PUT http://localhost:8500/v1/kv/foo?cas=2834
bar

# Wait for change (and for a time of 120s)
# (setq restclient-same-buffer-response t)
GET http://localhost:8500/v1/kv/foo?index=2866&wait=120s

# Services
GET http://localhost:8500/v1/agent/services


# Incomplete
# Session
PUT http://localhost:8500/v1/session/create
{
  "LockDelay": "60s",
  "Name": "ceph-service-lock",
  "Node": "node2",
  "Checks": ["service:ceph"]
}


## Leader Election
PUT http://localhost:8500/v1/kv/service/ceph/leader?d8b758ac-6810-a093-a839-76ee0969898c
node2

# Watch for the key changes
GET http://localhost:8500/v1/kv/service/ceph/leader

# Step down
PUT http://localhost:8500/v1/kv/service/ceph/leader?release=3eeab2e5-4c1a-fd92-8beb-9fcc795d8ee6
#+end_src

[1]: https://github.com/hashicorp/raft/commit/73bd785f4505fb27b97b253f37d40e4922d34227
#+begin_src go
- LeaderLeaseTimeout: time.Second,
+ LeaderLeaseTimeout: 300 * time.Millisecond,
#+end_src
y
