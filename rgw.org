#+TITLE: Rados Gateway
#+AUTHOR: Cloud IAAS Team
#+EMAIL: abhishek.lekshmanan@ril.com
#+REVEAL_TRANS: none
#+OPTIONS: reveal_progress

* Architecture
#+CAPTION: Basic rgw/rados interaction
#+header: :exports results
#+BEGIN_SRC ditaa :file images/rgw-top-level.png :cmdline -r

            +------------------------+ +------------------------+
            |   S3 compatible API    | |  Swift compatible API  |
            +------------------------+-+------------------------+
            |                      radosgw                      |
            +---------------------------------------------------+
            |                      librados                     |
            +------------------------+-+------------------------+
            |          OSDs          | |        Monitors        |
            +------------------------+ +------------------------+   


#+END_SRC

#+RESULTS:
[[file:images/rgw-top-level.png]]

Another client for RADOS
- Written in C++
- Rest Call -> libRados Call
- FCGI module providing S3 & Swift compatible API
- Other backends instead of fcgi also possible (civetweb,loadgen)
#+REVEAL: split

- Both S3 & Swift objects live in common namespace
- Objects stored into logical containers/buckets
- Stores data in dedicated pools 
  
  1   .rgw
  2   .rgw.root
  3   .rgw.control
  4   .rgw.gc
  5   .rgw.buckets
  6   .rgw.buckets.index
  7   .log
  8   .usage
  9   .users
  10  .users.email
  11  .users.swift
  12  .users.uid
 
** Buckets
- Sort of a namespace for objects 
- Similiar to Amazon S3
- Maps to containers in Swift


** Objects
- S3 or Swift objects may map to more than one striped Rados object
- RGW Object has 2 parts - an object logical head (olh) & an optional tail
- olh has a max size of 512K, tail split into chunks of stripe_width (4M:default)
- Technically you can join all the rados object to get back the object again
- Lets do a small experiment to find out

#+REVEAL: split

#+begin_src sh
[r@ra:~]$ cat /dev/urandom | strings --bytes 1 | tr -d '\n\t ' | head --bytes 8192K > random.txt
[r@ra:~]$ ls -lh random.txt
-+rw-rw-r-- 1 r r 8.0M Sep 17 18:08 random.txt
[r@ra:~/ceph/src](⎇ master)$ 
sha1sum random.txt 
61ea6dd7ff4a2de1e9ed12a43d60c2bbfa59b038  random.txt
[r@ra:~/ceph/src](⎇ master)$ s3 -us put my-first-bucket/random filename=random.txt
8372224 bytes remaining (0% complete) ...
294912 bytes remaining (96% complete) ...
...
#+end_src

#+REVEAL: split

- Object has the object manifest which gives info about the tail
- Using native librados client, we can get the object parts
- Rejoining these should give us the original object

#+REVEAL: split

#+begin_src sh
[r@ra:~/ceph/src](⎇ master)$ 
./radosgw-admin object stat --bucket=my-first-bucket --object=random  | grep prefix 
      "prefix": "._op2xmptte2DD7z3_9EjQKgmmRcWRWL_",
r@ra:~/ceph/src]$ ./rados get default.4124.1_random random.part0 --pool .rgw.buckets
[r@ra:~/ceph/src]$ ./rados get default.4124.1__shadow_._op2xmptte2DD7z3_9EjQKgmmRcWRWL_1 random.part1 --pool .rgw.buckets
[r@ra:~/ceph/src]$ ./rados get default.4124.1__shadow_._op2xmptte2DD7z3_9EjQKgmmRcWRWL_2 random.part2 --pool .rgw.buckets

# Now join the objects back 
[r@ra:~/ceph/src]$ cat random.part0 random.part1 random.part2 > random.rados.txt

# Verify we have the same object
[r@ra:~/ceph/src](⎇ master)$ 
sha1sum random.rados.txt 
61ea6dd7ff4a2de1e9ed12a43d60c2bbfa59b038  random.rados.txt
[r@ra:~/ceph/src](⎇ master)$ 
sha1sum random.txt 
61ea6dd7ff4a2de1e9ed12a43d60c2bbfa59b038  random.txt
#+end_src

* Geo Replication

** Rados replication
- Synchronous writes & Strong Consistency model of ceph

#+header: :exports results
#+BEGIN_SRC ditaa :file images/ceph-writes.png :cmdline -r
             +----------+
             |  Client  |
             |          |
             +----------+
                 *  ^
      Write (1)  |  |  Ack (6)
                 |  |
                 v  *
            +-------------+
            | Primary OSD |
            |             |
            +-------------+
              *  ^   ^  *
    Write (2) |  |   |  |  Write (3)
       +------+  |   |  +------+
       |  +------+   +------+  |
       |  | Ack (4)  Ack (5)|  | 
       v  *                 *  v
 +---------------+   +---------------+
 | Secondary OSD |   | Tertiary OSD  |
 |               |   |               |
 +---------------+   +---------------+

#+END_SRC

#+RESULTS:
[[file:images/ceph-writes.png]]

- Great for single DC deployments & nearby DCs etc.
- Long distances => high write latency
- No support in librados itself for a geo-replication model
- Hence implementation in RadosGW itself

** Zones
A zone is a *logical* grouping of one or more Ceph Object Gateway 
instance(s). A region has a master zone that processes client requests.

- Contains User + Bucket data , metadata rados pools etc 
- A set of rgw demons serving content 
   
** Region
A region represents a *logical* geographic area and contains one
or more zones. A cluster with multiple regions must specify a master region.

** Sync
- radosgw-agent provided to sync metadata (+ data) 

file:images/zone-sync.png

- Also metadata can be kept in sync across regions to ensure a unified ns

file:images/region-sync.png

* Misc
- Caching
- GC
